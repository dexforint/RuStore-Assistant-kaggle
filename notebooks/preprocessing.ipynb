{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docs.pickle', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "def save_image(base64_string, file_name):\n",
    "    # Удаляем префикс \"data:image/png;base64,\" если он есть\n",
    "    if 'base64,' in base64_string:\n",
    "        base64_string = base64_string.split('base64,')[1]\n",
    "    \n",
    "    # Декодируем base64 в бинарные данные\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    \n",
    "    # Сохраняем изображение\n",
    "    with open(f\"./images/{file_name}\", 'wb') as file:\n",
    "        file.write(image_data)\n",
    "\n",
    "\n",
    "\n",
    "def absolute_href(soup):\n",
    "    global img_counter\n",
    "    \n",
    "    a_tags = soup.find_all('a')\n",
    "\n",
    "    # Проверяем, найден ли тег\n",
    "    for a_tag in a_tags:\n",
    "        href = a_tag.get('href')\n",
    "        if href.startswith('/'):\n",
    "            a_tag['href'] = f\"https://www.rustore.ru{href}\"\n",
    "\n",
    "    images = soup.find_all('img')\n",
    "\n",
    "    for img in images:\n",
    "        src = img.get('src')\n",
    "\n",
    "        if src.startswith('/'):\n",
    "            img['src'] = f\"https://www.rustore.ru{src}\"\n",
    "        elif src.startswith('data:'):\n",
    "            save_image(src, f\"{img_counter:03}.png\")\n",
    "            img['src'] = f\"/images/{img_counter:03}.png\"\n",
    "\n",
    "            img_counter += 1\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_exceptions = {\n",
    "    'img': ['src', 'alt'],\n",
    "    'a': ['href'],\n",
    "    '*': ['id']\n",
    "}\n",
    "\n",
    "def remove_attrs(html_content):\n",
    "    global attr_exceptions\n",
    "\n",
    "    if isinstance(html_content, str):\n",
    "        # Создаем объект BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    else:\n",
    "        soup = html_content\n",
    "\n",
    "    \n",
    "    # Получаем список атрибутов, которые нужно сохранить для всех тегов\n",
    "    global_keep_attrs = attr_exceptions.get('*', [])\n",
    "    \n",
    "    # Находим все теги в документе\n",
    "    for tag in soup.find_all():\n",
    "        # Получаем имя тега\n",
    "        tag_name = tag.name\n",
    "        \n",
    "        # Получаем список атрибутов для сохранения (если есть)\n",
    "        keep_attrs = attr_exceptions.get(tag_name, []) + global_keep_attrs\n",
    "        \n",
    "        # Создаем список атрибутов для удаления\n",
    "        attrs_to_remove = [attr for attr in tag.attrs if attr not in keep_attrs]\n",
    "        \n",
    "        # Удаляем атрибуты\n",
    "        for attr in attrs_to_remove:\n",
    "            del tag[attr]\n",
    "    \n",
    "    # Возвращаем обновленный HTML\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tabs_to_sequential(soup):\n",
    "    # Создаем объект BeautifulSoup\n",
    "    # soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    tabs_containers = soup.find_all('div', class_='tabs-container')\n",
    "\n",
    "    for tab_container in tabs_containers:\n",
    "\n",
    "        # Находим все вкладки\n",
    "        tabs = tab_container.find_all('li', class_='tabs__item')\n",
    "\n",
    "        # Находим все панели вкладок\n",
    "        tab_panels = tab_container.find_all('div', attrs={'role': 'tabpanel'})\n",
    "\n",
    "        # Создаем новый div для последовательного контента\n",
    "        sequential_content = soup.new_tag('div', class_='sequential-content')\n",
    "\n",
    "        # Добавляем заголовки и содержимое каждой вкладки последовательно\n",
    "\n",
    "        max_panel_length = -1\n",
    "\n",
    "        for tab, panel in zip(tabs, tab_panels):\n",
    "            max_panel_length = max(max_panel_length, len(str(panel)))\n",
    "\n",
    "        for tab, panel in zip(tabs, tab_panels):\n",
    "            # Создаем заголовок\n",
    "            header = soup.new_tag('h2' if max_panel_length > 300 else 'h3')\n",
    "            header.string = tab.text.strip()\n",
    "            sequential_content.append(header)\n",
    "            \n",
    "            # Добавляем содержимое вкладки\n",
    "            sequential_content.append(panel)\n",
    "\n",
    "        # Заменяем исходный контейнер вкладок на последовательный контент\n",
    "        tab_container.replace_with(sequential_content)\n",
    "\n",
    "        # Удаляем атрибут hidden из всех панелей\n",
    "        for panel in sequential_content.find_all('div', attrs={'role': 'tabpanel'}):\n",
    "            del panel['hidden']\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def split_html(soup):\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Находим контейнер\n",
    "    container = soup.find()\n",
    "\n",
    "    # Находим все теги h1 и h2\n",
    "    headers = container.find_all(['h1', 'h2'])\n",
    "\n",
    "    # Создаем список для хранения частей\n",
    "    parts = []\n",
    "\n",
    "    # Проходим по всем заголовкам\n",
    "    for i, header in enumerate(headers):\n",
    "        # Если это первый заголовок (h1)\n",
    "        if i == 0:\n",
    "            # Собираем все элементы до следующего h2\n",
    "            content = [header]\n",
    "            for sibling in header.next_siblings:\n",
    "                if sibling.name == 'h2':\n",
    "                    break\n",
    "                content.append(sibling)\n",
    "            parts.append(''.join(str(elem) for elem in content))\n",
    "        else:\n",
    "            # Для h2 заголовков\n",
    "            content = [header]\n",
    "            for sibling in header.next_siblings:\n",
    "                if sibling.name == 'h2':\n",
    "                    break\n",
    "                content.append(sibling)\n",
    "            parts.append(''.join(str(elem) for elem in content))\n",
    "\n",
    "    # Выводим результат\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def replace_tables_with_text(soup, title=\"Строка таблицы\"):\n",
    "    for table in soup.find_all('table'):\n",
    "        # Получаем заголовки столбцов\n",
    "        headers = [header.text.strip() for header in table.find_all('th')]\n",
    "        \n",
    "        # Если заголовки не найдены, используем первую строку как заголовки\n",
    "        if not headers:\n",
    "            headers = [cell.text.strip() for cell in table.find('tr').find_all(['td', 'th'])]\n",
    "        \n",
    "        # Создаем новый элемент div для замены таблицы\n",
    "        table_replacement = soup.new_tag('div')\n",
    "        \n",
    "        # Обрабатываем каждую строку таблицы\n",
    "        for row in table.find_all('tr')[1:]:  # Пропускаем первую строку, если она содержит заголовки\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            \n",
    "            # Создаем заголовок для строки\n",
    "            row_header = soup.new_tag('h2')\n",
    "            row_header.string = title\n",
    "            table_replacement.append(row_header)\n",
    "            \n",
    "            # Добавляем информацию о каждом столбце\n",
    "            for header, cell in zip(headers, cells):\n",
    "                p = soup.new_tag('p')\n",
    "                p.string = f\"{header}: {cell.text.strip()}\"\n",
    "                table_replacement.append(p)\n",
    "        \n",
    "        # Заменяем таблицу на новый div\n",
    "        table.replace_with(table_replacement)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 461/461 [00:18<00:00, 24.69it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "html_splits = []\n",
    "texts = set()\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    html = doc['html']\n",
    "    html = html.replace(\"\\x00\", \"\")\n",
    "    url = doc['url']\n",
    "    section = \" > \".join(doc[\"breadcrumbs\"])\n",
    "\n",
    "    soup = BeautifulSoup(html,  'html.parser')\n",
    "\n",
    "    page_title = soup.find('h1').text.strip()\n",
    "\n",
    "    # [table.extract() for table in soup.find_all('table')]\n",
    "    soup = replace_tables_with_text(soup, section)\n",
    "    soup = BeautifulSoup(str(soup),  'html.parser')\n",
    "\n",
    "    soup  = convert_tabs_to_sequential(soup)\n",
    "\n",
    "    # Находим все теги <p>, содержащие <strong>\n",
    "    for p in soup.find_all('p'):\n",
    "        strong = p.find('strong')\n",
    "        if strong and strong.parent == p and len(p.contents) == 1:\n",
    "            # Создаем новый тег <h2>\n",
    "            h2 = soup.new_tag('h2')\n",
    "            h2.string = strong.text\n",
    "            # Заменяем <p><strong> на <h2>\n",
    "            p.replace_with(h2)\n",
    "    \n",
    "    soup = remove_attrs(soup)\n",
    "\n",
    "    soup = absolute_href(soup)\n",
    "\n",
    "    parts = split_html(soup)\n",
    "\n",
    "\n",
    "    for i, part in enumerate(parts):\n",
    "        soup = BeautifulSoup(part,  'html.parser')\n",
    "\n",
    "        h2 = soup.find('h2')\n",
    "\n",
    "        if h2:\n",
    "            title = h2.text.strip()\n",
    "            a = h2.find('a')\n",
    "            if not (a is None):\n",
    "                a.decompose()\n",
    "        else:\n",
    "            title = page_title\n",
    "\n",
    "        text = soup.get_text()\n",
    "\n",
    "        if text in texts:\n",
    "            continue\n",
    "\n",
    "        texts.add(text)\n",
    "\n",
    "        root = soup.find()\n",
    "        tag_id = root.get('id')\n",
    "        if tag_id:\n",
    "            anchor = f\"#{tag_id}\"\n",
    "        else:\n",
    "            anchor = \"\"\n",
    "\n",
    "        if (len(part) - len(page_title)) < 300:\n",
    "            continue\n",
    "\n",
    "        html_splits.append({\n",
    "            \"page_title\": page_title,\n",
    "            \"title\": title,\n",
    "            \"section\": section,\n",
    "            \"html\": str(soup),\n",
    "            \"text\": text,\n",
    "            \"url\": url + anchor \n",
    "        })\n",
    "len(html_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"html_splits.pickle\", 'wb') as file:\n",
    "    pickle.dump(html_splits, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
